{
  
    
        "post0": {
            "title": "Anomaly detection in protocol deviations",
            "content": "Deviations from the approved protocol of a clinical trial are common but need to be reported by investigators to the trial sponsor for review. Failing to do so can put patients at risk and affect the scientific quality of the trial, so detecting protocol deviation underreporting is part of the standard quality assurance activities. Investigator sites that report more deviations than their peers should also raise an alarm as this might indicate underlying quality issues. A statistical tool that quantifies the risk of over- and underreporting of protocol deviations could significantly improve quality activities, much like in the case of adverse event reporting. . The number of protocol deviations $n_{pdevs}$ reported by an investigator site should depend linearly on the number of patients $n_{pats}$ enrolled at that site, as more patients mean more chances of deviations, so these are the minimal attributes that should get collected prior to the analysis. The following table is a sample of the data we will use as an example. . site_id n_pats n_pdevs . 0 site_0 | 9 | 15 | . 1 site_1 | 10 | 14 | . 2 site_2 | 15 | 18 | . 3 site_3 | 9 | 10 | . 4 site_4 | 5 | 4 | . A quick glance at the full dataset reveals there is indeed a relationship that looks linear, so a potential approach would be to build a regression model $n_{pdevs} sim theta cdot n_{pats}$ and quantify how every observation of $n_{pdevs}$ deviates from its estimation. . From that scatterplot, it also appears that the residuals of a regression would not be iid. Rather, their variance would grow as the number of patients increases. This rules out least square regression, as it assumes iid normal residuals. Since we are dealing with count data, working with the Poisson distribution is a natural approach, . $$n_{pdevs} vert n_{pats} sim Poi( lambda(n_{pats})),$$ . and we can set $ lambda(n_{pats}) = theta cdot n_{pats}$ to reflect our assumption of a linear relationship between $n_{pdevs}$ and $n_{pats}$. Note that a regular Poisson regression would fail to capture that linear relationship due to its exponential link function. . In this model, we immediately have $E left[n_{pdevs} vert n_{pats} right] = Var left[n_{pdevs} vert n_{pats} right] = theta cdot n_{pats}$, which seems to reproduce the increasing spread of $n_{pdevs}$. . We can infer the value of $ theta$ through maximum likelihood estimation and use the resulting conditional Poisson model at each site to compute the cumulative distribution function (CDF) of the observed numbers of protocol deviations. . def loss(par, n_pat, n_dev): theta = tf.math.exp(par[0]) dist = tfd.Poisson(n_pat * theta) return -tf.reduce_sum(dist.log_prob(n_dev)) def compute_cdf(par, n_pat, n_dev): theta = tf.math.exp(par[0]) dist = tfd.Poisson(n_pat * theta) return dist.cdf(n_dev) @tf.function def loss_and_gradient(par, n_pat, n_dev): return tfp.math.value_and_gradient(lambda par: loss(par, n_pat, n_dev), par) def fit(n_pat, n_dev): init = 2*tf.ones(1) opt = tfp.optimizer.lbfgs_minimize( lambda par: loss_and_gradient(par, n_pat, n_dev), init, max_iterations=1000 ) return opt n_pats = tf.constant(data[&#39;n_pats&#39;], dtype=tf.float32) n_pdev = tf.constant(data[&#39;n_pdevs&#39;], dtype=tf.float32) mle = fit(n_pats, n_pdev) #print(f&quot;converged: {mle.converged}&quot;) #print(f&quot;iterations: {mle.num_iterations}&quot;) x = np.linspace(0, 40) par = mle.position y = np.exp(par[0]) * x cdfs = compute_cdf(par, n_pats, n_pdev) . . These CDF values are concentrated around 0 and 1, which makes this approach quite impractical and suggests that the variance of the model is lower than the variance of the data. . The low variance can be increased by treating $ lambda(n_{pats})$ as a random function, rather than a deterministic one. So we assume that $ lambda(n_{pats})$ is drawn from a gamma distribution, $ lambda(n_{pats}) sim Gamma( alpha, beta)$, where the rate parameter $ beta$ is inversely proportional to the expected number of protocol deviations from a given site, $ beta = beta_{pat} / n_{pats}$, in order to ensure linearity in $n_{pats}$. In this context, maximum likelihood estimation would be a nightmare to implement (because of the rate parameters of the Poisson distribution) and probably not very stable, so it is best to turn to Bayesian inference via MCMC algorithms. We thus pick gamma priors for $ alpha$ and $ beta_{pat}$ with a shape parameters of 2 to prevent the corresponding Markov chains from drifting too close to zero, where pathological behaviors seem to occur with more permissive priors in this model. . sites = tf.constant(data[&#39;site_id&#39;]) n_pats = tf.constant(data[&#39;n_pats&#39;], dtype=tf.float32) n_pdev = tf.constant(data[&#39;n_pdevs&#39;], dtype=tf.float32) mdl_pd = tfd.JointDistributionSequential([ #alpha tfd.Gamma(2, 2, name=&#39;alpha&#39;), #beta_pt tfd.Gamma(2, 2, name=&#39;beta_pt&#39;), #pdev rates for each patient lambda beta_pt, alpha: tfd.Independent( tfd.Gamma(alpha[...,tf.newaxis], beta_pt[...,tf.newaxis] / n_pats[tf.newaxis,...]), reinterpreted_batch_ndims=1 ), #observed pdevs lambda rates: tfd.Independent(tfd.Poisson(rates), reinterpreted_batch_ndims=1) ]) . . We can sample the posterior distribution of this model with a Hamiltonian Monte Carlo algorithm and assess the convergence of the Markov chains before computing the posterior probabilities of interest. . dtype = tf.dtypes.float32 nchain = 5 burnin=3000 num_steps=10000 alpha0, beta_pt0, rates0, _ = mdl_pd.sample(nchain) init_state = [alpha0, beta_pt0, rates0] step_size = [tf.cast(i, dtype=dtype) for i in [0.01, 0.01, 0.01]] target_log_prob_fn = lambda *init_state: mdl_pd.log_prob( list(init_state) + [tf.cast(n_pdev, dtype=dtype)]) unconstraining_bijectors = 3*[tfb.Exp()] @tf.function(autograph=False, experimental_compile=True) def run_chain(init_state, step_size, target_log_prob_fn, unconstraining_bijectors, num_steps=num_steps, burnin=burnin): def trace_fn(_, pkr): return ( pkr.inner_results.inner_results.is_accepted ) kernel = tfp.mcmc.TransformedTransitionKernel( inner_kernel=tfp.mcmc.HamiltonianMonteCarlo( target_log_prob_fn, num_leapfrog_steps=3, step_size=step_size), bijector=unconstraining_bijectors) hmc = tfp.mcmc.SimpleStepSizeAdaptation( inner_kernel=kernel, num_adaptation_steps=burnin ) # Sampling from the chain. [alpha, beta_pt, rates], is_accepted = tfp.mcmc.sample_chain( num_results=num_steps, num_burnin_steps=burnin, current_state=init_state, kernel=hmc, trace_fn=trace_fn) return alpha, beta_pt, rates, is_accepted alpha, beta_pt, rates, is_accepted = run_chain( init_state, step_size, target_log_prob_fn, unconstraining_bijectors) alpha_ = alpha[burnin:,:] alpha_ = tf.reshape(alpha_, [alpha_.shape[0]*alpha_.shape[1]]) beta_pt_ = beta_pt[burnin:,:] beta_pt_ = tf.reshape(beta_pt_, [beta_pt_.shape[0]*beta_pt_.shape[1]]) rates_ = rates[burnin:,:] rates_ = tf.reshape(rates_, [rates_.shape[0]*rates_.shape[1], rates_.shape[2]]) rates_dist_ = tfd.Gamma(alpha_[:,tf.newaxis], beta_pt_[:, tf.newaxis] / n_pats[tf.newaxis,...]) rates_cdf_ = rates_dist_.cdf(rates_) posterior = {} posterior[&#39;alpha&#39;] = tf.transpose(alpha[burnin:, :]).numpy() posterior[&#39;beta_pt&#39;] = tf.transpose(beta_pt[burnin:, :]).numpy() posterior[&#39;rate0&#39;] = tf.transpose(rates[burnin:, :, 0]) posterior[&#39;rate1&#39;] = tf.transpose(rates[burnin:, :, 1]) posterior[&#39;rate2&#39;] = tf.transpose(rates[burnin:, :, 2]) az_trace = az.from_dict(posterior=posterior) print(f&#39;MCMC acceptance rate: {is_accepted.numpy().mean()}&#39;) az.plot_trace(az_trace) plt.show() az.summary(az_trace) . . MCMC acceptance rate: 0.7745 . mean sd hdi_3% hdi_97% mcse_mean mcse_sd ess_mean ess_sd ess_bulk ess_tail r_hat . alpha 1.35 | 0.19 | 1.00 | 1.71 | 0.00 | 0.00 | 1,838.00 | 1,838.00 | 1,810.00 | 7,005.00 | 1.00 | . beta_pt 0.51 | 0.08 | 0.35 | 0.66 | 0.00 | 0.00 | 3,012.00 | 3,012.00 | 2,930.00 | 8,825.00 | 1.00 | . rate0 15.51 | 3.83 | 8.62 | 22.74 | 0.05 | 0.04 | 5,800.00 | 5,800.00 | 5,545.00 | 8,565.00 | 1.00 | . rate1 14.52 | 3.74 | 7.58 | 21.30 | 0.05 | 0.04 | 5,415.00 | 5,415.00 | 5,203.00 | 7,654.00 | 1.00 | . rate2 18.84 | 4.28 | 11.24 | 27.06 | 0.05 | 0.04 | 6,743.00 | 6,743.00 | 6,539.00 | 9,273.00 | 1.00 | . Given a Markov chain sample $( hat alpha, hat beta_{pat}, ( hat lambda_i)_{i=1, dots,N})$, where $i$ indexes the investigator sites, we can evaluate the CDF of $ Gamma( hat alpha, hat beta_{pat} / n_{pats, i})$ at $ hat lambda_i$ and average these quantities along the whole Markov chain to obtain an indicator of over- and underreporting. This indicator corresponds to the rate tail area of the inferred Poisson rates under their posterior predictive distribution. Low values mean a risk of underreporting, and high values a risk of overreporting (see the last column of the following sample table). . site n_pats n_pdev mean_pdev_rate std_pdev_rate rate_tail_area . 0 site_0 | 9 | 15 | 15.51 | 3.83 | 0.42 | . 1 site_1 | 10 | 14 | 14.52 | 3.74 | 0.36 | . 2 site_2 | 15 | 18 | 18.83 | 4.28 | 0.31 | . 3 site_3 | 9 | 10 | 10.72 | 3.20 | 0.30 | . 4 site_4 | 5 | 4 | 4.96 | 2.14 | 0.24 | . 5 site_5 | 7 | 19 | 19.03 | 4.21 | 0.61 | . 6 site_6 | 25 | 31 | 31.76 | 5.60 | 0.32 | . 7 site_7 | 2 | 11 | 9.90 | 2.83 | 0.83 | . 8 site_8 | 6 | 7 | 7.62 | 2.69 | 0.31 | . 9 site_9 | 17 | 27 | 27.58 | 5.14 | 0.40 | . The distribution of the rate tail areas looks more convenient than in the first simple model. Not only did we add variance with a mixture model, but we also assess the inferred Poisson parameters rather than the observations, and the former are shrunk by their prior. . This metric also seems to agree with the intuition of what underreporting and overreporting should look like. . We can set thresholds for over- and underreporting alerts at .8 and .2 respectively to illustrate how an auditor could use this model to select investigator sites to focus on. . This method further illustrates the potential of Bayesian analysis to address business problems in a systematic way that combines insights from data with subject matter expertise. Moreover, I found that such models are much easier to explain to stakeholders than black box machine learning, which is a big plus when you advocate for quantitative decision making. .",
            "url": "https://ybarmaz.github.io/blog/bayesian%20modeling/clinical%20quality/2021/01/13/Anomaly-detection-in-protocol-deviations.html",
            "relUrl": "/bayesian%20modeling/clinical%20quality/2021/01/13/Anomaly-detection-in-protocol-deviations.html",
            "date": " • Jan 13, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Probabilistic assessment of safety underreporting",
            "content": "This post presents a practical application of probabilistic programming and explores some technical details of a recent preprint where Timothé Ménard and I proposed a Bayesian model for the detection of adverse event underreporting in clinical trials. . A clinical trial is usually run across several investigator sites, which are required to report to the trial sponsor adverse events experienced by enrolled subjects. This is necessary to assess the safety of the intervention under investigation. Underreporting from certain sites has been a recurrent issue. Trial sponsors and health authorities rely on audits and inspections to ensure completeness of the collected safety data. This effort can and should be informed by statistical analysis of the adverse event reporting process with a focus on identifying sites that report at lower rates. . The main hurdle is that you are looking for missing data that you do not know is missing. We developed a solution around an observational model for adverse event reporting and a way to characterizing outlying investigator sites. . The relevant data is the count of adverse events reported by every patient enrolled in a study and their site assignment. To illustrate our methodology, we use data from the control arm of NCT00617669, an oncology study from Project Data Sphere. . Bayesian data analysis combines expert knowledge on the process of interest, expressed as a probabilistic model, with observed data to infer posterior distributions of the model parameters. These posterior distributions allow us to quantify uncertainties and risks, and to compute posterior expectation values of quantities of interest. . Hierarchical models provide a natural framework for subgroup analysis, where similar entities such as the sites of a single study can share information while maintaining a certain degree of independence. In our situation, at the bottom of the hierarchy, the count of adverse events reported by the $n_i$ patients of site $i$ can be modelled with a Poisson distribution, $Y_i sim mathrm{Poi}( lambda_i)$. The $N_{ mathrm{sites}}$ Poisson rates $ lambda_i$ can in turn be modelled as realizations of a random variable unique to the whole study with Gamma distribution $ Gamma( alpha, beta)$. The parameters $ alpha$ and $ beta$ are unknown, so we assume a vague prior for both of them, $ alpha sim mathrm{Exp}(1)$ and $ beta sim mathrm{Exp}(10)$. The full joint distribution $$ P( alpha, beta, lambda_i, Y_{i,j}) = P( alpha)P( beta) prod_{i=1}^{N_{ mathrm{sites}}}P( lambda_i vert alpha, beta) prod_{j=1}^{n_i} P(Y_{i,j} vert lambda_i) $$ is summarized in the following graphical representation. . . Numerical modelling of such joint distributions is made easy by probabilistic programming libraries such as TensorFlow-Probability (utilized here), Stan, PyMC3 or Pyro. . import tensorflow as tf import tensorflow_probability as tfp tfb = tfp.bijectors tfd = tfp.distributions sites = tf.constant(data[&#39;site_number&#39;]) observed_ae = tf.constant(data[&#39;ae_count_cumulative&#39;]) unique_sites, sites_idx, sites_counts = tf.unique_with_counts(sites) ae_per_site = tf.RaggedTensor.from_value_rowids( values=observed_ae, value_rowids=sites_idx) mdl_ae = tfd.JointDistributionSequential([ #alpha tfd.Gamma(1, 1, name=&#39;alpha&#39;), #beta tfd.Gamma(1, 10, name=&#39;beta&#39;), #Poisson rates for each sites lambda beta, alpha: tfd.Sample(tfd.Gamma(alpha, beta), sample_shape=unique_sites.shape, name=&#39;rates&#39;), #observed AEs lambda rates: tfd.Independent( tfd.Poisson(tf.gather(rates, sites_idx, axis=-1)), reinterpreted_batch_ndims=1, name=&#39;observations&#39;) ]) . In Bayesian inference, the analytical derivation of the posterior distribution is often impossible, but the same probabilistic programming libraries provide efficient implementations of MCMC algorithms that return samples of the posterior distribution. . dtype = tf.dtypes.float32 nchain = 10 burnin=1000 num_steps=10000 alpha0, beta0, rates0, _ = mdl_ae.sample(nchain) init_state = [alpha0, beta0, rates0] step_size = [tf.cast(i, dtype=dtype) for i in [.1, .1, .1]] target_log_prob_fn = lambda *init_state: mdl_ae.log_prob( list(init_state) + [tf.cast(observed_ae, dtype=dtype)]) # bijector to map contrained parameters to real unconstraining_bijectors = [ tfb.Exp(), tfb.Exp(), tfb.Exp() ] @tf.function(autograph=False, experimental_compile=True) def run_chain(init_state, step_size, target_log_prob_fn, unconstraining_bijectors, num_steps=num_steps, burnin=burnin): def trace_fn(_, pkr): return ( pkr.inner_results.inner_results.is_accepted ) kernel = tfp.mcmc.TransformedTransitionKernel( inner_kernel=tfp.mcmc.HamiltonianMonteCarlo( target_log_prob_fn, num_leapfrog_steps=3, step_size=step_size), bijector=unconstraining_bijectors) hmc = tfp.mcmc.SimpleStepSizeAdaptation( inner_kernel=kernel, num_adaptation_steps=burnin ) # Sampling from the chain. [alpha, beta, rates], is_accepted = tfp.mcmc.sample_chain( num_results=num_steps, num_burnin_steps=burnin, current_state=init_state, kernel=hmc, trace_fn=trace_fn) return alpha, beta, rates, is_accepted . . To assess convergence, we sample several chains that we can inspect visually (here with the ArviZ package) to make sure that they converge to the same distribution, mix well, and do not display pathological autocorrelations. . If we have to monitor several studies, we might want to automate that process. In that case, we can compute statistics of the sampled chains such as effective sample sizes or $ hat{R}$ and implement automatic checks, for instance that $ hat{R}$ is sufficiently close to 1. . mean sd hdi_3% hdi_97% mcse_mean mcse_sd ess_mean ess_sd ess_bulk ess_tail r_hat . 3001 3.193 | 1.246 | 1.081 | 5.565 | 0.022 | 0.016 | 3146.0 | 3146.0 | 2849.0 | 3764.0 | 1.0 | . 3002 3.050 | 0.616 | 1.964 | 4.250 | 0.006 | 0.004 | 11787.0 | 11787.0 | 11502.0 | 17056.0 | 1.0 | . 3003 6.043 | 1.695 | 3.084 | 9.280 | 0.022 | 0.015 | 6031.0 | 6031.0 | 5772.0 | 9765.0 | 1.0 | . 3004 12.764 | 2.030 | 9.037 | 16.628 | 0.014 | 0.010 | 19819.0 | 19819.0 | 19489.0 | 28664.0 | 1.0 | . alpha 1.774 | 0.223 | 1.364 | 2.192 | 0.003 | 0.002 | 6352.0 | 6352.0 | 6253.0 | 16128.0 | 1.0 | . beta 0.119 | 0.017 | 0.087 | 0.151 | 0.000 | 0.000 | 7576.0 | 7576.0 | 7441.0 | 18742.0 | 1.0 | . From the samples $( hat{ alpha}, hat{ beta}, hat{ lambda}_i)$ of the Markov chain, we can estimate the posterior risk of underreporting. One way to do it is to compute the left tail area of each $ lambda_i$ (remember that the index $i$ enumerates the sites) under the distribution $ Gamma( hat{ alpha}, hat{ beta})$ and average it along the trace of the Markov chain. This corresponds to the probability that a Poisson rate drawn randomly from the study level distribution falls below the inferred Poisson rate of site $i$, or, more explicitly, that a reference site from the same study would report less adverse events. . site mean_ae_rate std_ae_rate rate_tail_area observed_ae . 0 3001 | 3.20 | 1.25 | 0.09 | [4, 1] | . 1 3002 | 3.05 | 0.62 | 0.08 | [2, 2, 1, 2, 5, 5, 5, 1] | . 2 3003 | 6.04 | 1.69 | 0.22 | [7, 4] | . 3 3004 | 12.76 | 2.03 | 0.52 | [3, 27, 8] | . 4 3005 | 9.35 | 2.10 | 0.37 | [12, 6] | . 5 3006 | 3.69 | 1.32 | 0.11 | [2, 4] | . 6 3007 | 6.03 | 2.31 | 0.22 | [5] | . 7 3008 | 17.95 | 1.72 | 0.69 | [11, 4, 16, 31, 23, 23] | . 8 3009 | 6.52 | 1.76 | 0.24 | [6, 6] | . 9 3010 | 14.71 | 0.92 | 0.59 | [21, 10, 6, 17, 10, 7, 26, 19, 18, 1, 18, 23, ... | . A lower value of this rate tail area thus indicates a higher risk of underreporting. This metric can be used by auditors and inspectors to prioritize their activities. Moreover, since it is the probability of a specified event, one can immediately compare the rate tail areas of sites from different studies. This is especially interesting for quality programs that oversee several trials. . This approach demonstrates the flexibility of Bayesian methods to build models that answer specific questions about a given process. In this example, the reporting rates of the different sites are the quantities of interest, but they are unobserved and have to be inferred from the available data with a mathematical model. The user-friendly API of modern probabilistic programming libraries combined with efficient inference algorithms have been making this type of workflow much easier than in the past and will certainly fuel a broader adoption in sectors that have not been traditionally driven by quantitative insights. .",
            "url": "https://ybarmaz.github.io/blog/bayesian%20modeling/clinical%20quality/2021/01/08/Probabilistic-assessment-of-safety-underreporting.html",
            "relUrl": "/bayesian%20modeling/clinical%20quality/2021/01/08/Probabilistic-assessment-of-safety-underreporting.html",
            "date": " • Jan 8, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "My background is in theoretical physics (quantum fields and strings) and I now work as a data scientist in the pharmaceutical industry, focusing mostly on applications of quantitative methods to drug development and optimization of business processes. . In my spare time, I enjoy ski touring, mountain biking and cycling, especially in the Swiss Alps where I grew up. . In this blog, I am sharing learnings from my data science and applied mathematics journey, so hopefully you won’t get stuck to the same sticky places. . . Get in touch . You can email me at yves.barma@gmail.com if you add the missing z before @. . Selected publications . Bayesian modeling for the detection of adverse events underreporting in clinical trials (preprint, submitted to Drug Safety) | Enabling Data-Driven Clinical Quality Assurance: Predicting Adverse Event Reporting in Clinical Trials Using Machine Learning | Using Statistical Modeling for Enhanced and Flexible Pharmacovigilance Audit Risk Assessment and Planning | Chern-Simons Theory with Wilson Lines and Boundary in the BV-BFV Formalism | . . This website is powered by fastpages by Hamel Husain. .",
          "url": "https://ybarmaz.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://ybarmaz.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}