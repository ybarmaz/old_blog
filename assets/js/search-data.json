{
  
    
        "post0": {
            "title": "Thompson sampling for contextual bandits",
            "content": "The multi-armed bandit problem is inspired by the situation of gamblers facing $N$ slot machines with a limited amount of resources to &quot;invest&quot; in them, without knowing the probability distribution of rewards from each machine. By playing with a machine, they can of course sample its distribution. Once they find a machine that performs well enough, the question is wheter they should try the other ones that might perform even better, at the risk of wasting money because they might be worse. This is an example of the exploration-exploitation tradeoff dilemma. Applications include clinical trial design, portfolio selection, and A/B testing. . Thompson sampling is an approximate solution applicable to bandits for which we have a Bayesian model of the reward $r$, namely a likelihood $P(r vert theta, a)$ that depends on the action $a$ (the choice of an arm to pull) and a vector of parameters $ theta$, and a prior distribution $P( theta)$. In certain cases, called contextual bandits, the likelihood also depends on a set of features $x$ observed by the players before they choose an action, $P(r vert theta, a, x)$. After each round, the posterior distribution $P( theta vert left lbrace r_i, a_i, x_i right rbrace_{i})$ is updated with the newly observed data. Then a $ theta^ ast$ is sampled from it, the new context $x$ is observed, and the new action is chosen to maximize the expected reward, $a^ ast = mathrm{argmax}_a mathbb{E}(r vert theta, a, x)$. . This approach solves the exploration-exploitation dilemma with the random sampling of $ theta^ ast$, which gives to every action a chance to be selected, yet favors the most promising ones. The more data is collected, the more informative the posterior distribution will become and the more it will favor its top performer. . This mechanism is illustrated in the chapter 6 of Probabilistic Programming &amp; Bayesian Methods for Hackers and the section 4.4 of Bayesian Adaptive Methods for Clinical Trials. . Both discuss the case of a binary reward (success and failure) for every action $a$ that follows a Bernoulli distribution with unknown probability of success $p_a$. They assume a beta prior for each of the $p_a$, which is the conjugate prior for the Bernoulli likelihood and makes inference of the posterior straightforward. This is particularly appealing when you have to update your posterior after each play. . If there are covariates that can explain the probability of success, one of the simplest models for a binary response of the potential actions is the combination of generalized linear models for each action, . $$ P(r=1 vert theta, a, x) = frac{1}{1 + e^{- left( alpha_a + beta_a^T ,x right)}} $$Unfortunately, there is no immediate congugate prior for this type of likelihood, so we have to rely on numerical methods to estimate the posterior distribution. A previous blog post discussed variational inference as a speedier alternative to MCMC algorithms, and we will see here how we can apply it to the problem of contextual bandits with binary response. . This problem is relevant in the development of personalized therapies, where the actions represent the different treatments under investigation and the contexts $x$ are predictive biomarkers of their response. The goal of a trial would be to estimate the response to each treatment option given biomarkers $x$, and, based on that, to define the best treatment policy. Adaptive randomization through Thompson sampling ensures that more subjects enrolled in the trial get the optimal treatment based on their biomarkers and the knowledge accrued until their randomization, which is certainly more ethical than a randomization independent on the biomarkers. . Another example is online ad serving, where the binary response corresponds to a successful conversion, the action is the selection of an ad for a specific user, and the context is a set of features related to that user. When a new ad enters the portfolio and a new click-through rate model needs to be deployed for it, Thompson sampling can accelerate the training phase and reduce the related costs. . Bandit model . For simplicity, we simulate bandits whose true probabilities of success follow logistic models, so we can see how the posterior distributions concentrate around the true values during training. You can run this notebook in Colab to experiment with more realistic models, and vary the number of arms or the context dimension. . class ContextualBandit(object): &quot;&quot;&quot; This class represents contextual bandit machines with n_arms arms and linear logits of p-dimensional contexts. parameters: arm_true_weights: (n_arms, p) Numpy array of real weights. arm_true_biases: (n_arms,) Numpy array of real biases methods: pull( arms, X ): returns the results, 0 or 1, of pulling the arms[i]-th bandit given an input context X[i]. arms is an (n,) array of arms indices selected by the player and X an (n, p) array of contexts observed by the player before making a choice. get_logits(X): returns the logits of all bandit arms for every context in the (n, p) array X get_probs(X): returns sigmoid(get_logits(X)) get_selected_logits(arms, X): returns from get_logits(X) only the logits corresponding to the selected arms get_selected_probs(arms, X): returns sigmoid(get_selected_logits(arms, X)) get_optimal_arm(X): returns the arm with the highest probability of success for every context in X &quot;&quot;&quot; def __init__(self, arm_true_weights, arm_true_biases): self._arm_true_weights = tf.convert_to_tensor( arm_true_weights, dtype=tf.float32, name=&#39;arm_true_weights&#39;) self._arm_true_biases = tf.convert_to_tensor( arm_true_biases, dtype=tf.float32, name=&#39;arm_true_biases&#39;) self._shape = np.array( self._arm_true_weights.shape.as_list(), dtype=np.int32) self._dtype = tf.convert_to_tensor( arm_true_weights, dtype=tf.float32).dtype.base_dtype @property def dtype(self): return self._dtype @property def shape(self): return self._shape def get_logits(self, X): return tf.matmul(X, self._arm_true_weights, transpose_b=True) + self._arm_true_biases def get_probs(self, X): return tf.math.sigmoid(self.get_logits(X)) def get_selected_logits(self, arms, X): all_logits = self.get_logits(X) column_indices = tf.convert_to_tensor(arms, dtype=tf.int64) row_indices = tf.range(X.shape[0], dtype=tf.int64) full_indices = tf.stack([row_indices, column_indices], axis=1) selected_logits = tf.gather_nd(all_logits, full_indices) return selected_logits def get_selected_probs(self, arms, X): return tf.math.sigmoid(self.get_selected_logits(arms, X)) def pull(self, arms, X): selected_logits = self.get_selected_logits(arms, X) return tfd.Bernoulli(logits=selected_logits).sample() def pull_all_arms(self, X): logits = self.get_logits(X) return tfd.Bernoulli(logits=logits).sample() def get_optimal_arm(self, X): return tf.argmax( self.get_logits(X), axis=-1) . . Here we work with a two-dimensional context drawn from two independent standard normal distributions, and we select true weights and biases that correspond to an overall probability of success of about 30% for each arm, a situation that might be encountered in a personalized medicine question. . true_weights = np.array([[2., 0.],[0., 3.]]) true_biases = np.array([-1., -2.]) N_ARMS = true_weights.shape[0] CONTEXT_DIM = true_weights.shape[1] bandit = ContextualBandit(true_weights, true_biases) population = tfd.Normal(loc=tf.zeros(CONTEXT_DIM, dtype=tf.float32), scale=tf.ones(CONTEXT_DIM, dtype=tf.float32)) . Thompson sampling . A Thompson sampler based on a logistic regression can be implemented as a generalization of the probabilistic machine learning model discussed in the previous post. It is essentially a single dense variational layer with one unit per arm of the contextual bandit we want to solve. These units are fed into a Bernoulli distribution layer that simulates the pull of each arm. . The parameters $ theta$ of the model are encoded in the posterior_mean_field used as a variational family for the dense variational layer, and when we fit the full model to data, it converges to an approximation of the true posterior $P( theta vert left lbrace r_i, a_i, x_i right rbrace_{i})$. . A subsequent call of that dense variational layer on a new input $x$ will return random logits drawn from the approximate posterior predictive distribution and can thus be used to implement Thompson sampling (see the randomize method in the code). The $a^ ast = mathrm{argmax}_a mathbb{E}(r vert theta, a, x)$ step is the selection of the unit with the highest logit. . For training, the loss function is the negative log-likelihood of the observed outcome $r_i$, but only for the unit corresponding to the selected action $a_i$, so it is convenient to combine them into a unique output $y_i=(a_i,r_i)$ and write a custom loss function. . class ThompsonLogistic(tf.keras.Model): &quot;&quot;&quot; This class represents a Thompson sampler for a Bayesian logistic regression model. It is essentially a keras Model of a single layer Bayesian neural network with Bernoulli output enriched with a Thompson randomization method that calls only the dense variational layer. Parameters: - context_dim: dimension of the context - n_arms: number of arms of the multi-arm bandit under investigation - sample_size: size of the current training set of outcome observations, used to scale the kl_weight of the dense variational layer Methods: - randomize(inputs): returns a logit for each arm drawn from the (approximate) posterior predictive distribution - get_weights_stats(): returns means and sttdevs of the surrogate posterior of the model parameters - predict_probs(X, sample_size): returns the posterior probability of success for each context in the array X and each arm of the bandit, sample_size specifies the sample size of the Monte Carlo estimate - assign_best_mc(X, sample_size): returns the arms with the highest predict_probs(X, sample_size) - assign_best(X): returns the arms with the highest expected logit, should be very similar to assign_best_mc, a little bit less accurate &quot;&quot;&quot; def __init__(self, context_dim, n_arms, sample_size): super().__init__() self.context_dim = context_dim self.n_arms = n_arms self.densevar = tfp.layers.DenseVariational(n_arms, posterior_mean_field, prior_ridge, kl_weight=1/sample_size) self.bernoullihead = tfp.layers.DistributionLambda(lambda t: tfd.Bernoulli(logits=t)) def call(self, inputs): x = self.densevar(inputs) return self.bernoullihead(x) def randomize(self, inputs): return self.densevar(inputs) def get_weights_stats(self): n_params = self.n_arms * (self.context_dim + 1) c = np.log(np.expm1(1.)) weights = self.densevar.weights[0] means = weights[:n_params].numpy().reshape(self.context_dim + 1, self.n_arms) stddevs = (1e-5 + tf.nn.softplus(c + weights[n_params:])).numpy().reshape(self.context_dim + 1, self.n_arms) mean_weights = means[:-1] mean_biases = means[-1] std_weights = stddevs[:-1] std_biases = stddevs[-1] return mean_weights, mean_biases, std_weights, std_biases def assign_best(self, X): mean_weights, mean_biases, std_weights, std_biases = self.get_weights_stats() logits = tf.matmul(X, mean_weights) + mean_biases return tf.argmax(logits, axis=1) def predict_probs(self, X, sample_size=100): mean_weights, mean_biases, std_weights, std_biases = self.get_weights_stats() weights = tfd.Normal(loc=mean_weights, scale=std_weights).sample(sample_size) biases = tfd.Normal(loc=mean_biases, scale=std_biases).sample(sample_size) probs = tf.math.sigmoid(tf.matmul(X, weights)+biases[:,tf.newaxis,:]) return tf.reduce_mean(probs, axis=0) def assign_best_mc(self, X, sample_size=100): probs = self.predict_probs(X, sample_size) return tf.argmax(probs, axis=1) # Specify the surrogate posterior over `keras.layers.Dense` `kernel` and `bias`. def posterior_mean_field(kernel_size, bias_size=0, dtype=None): n = kernel_size + bias_size c = np.log(np.expm1(1.)) return tf.keras.Sequential([ tfp.layers.VariableLayer(2 * n, initializer=tfp.layers.BlockwiseInitializer([ &#39;zeros&#39;, tf.keras.initializers.Constant(np.log(np.expm1(.7))), ], sizes=[n, n]), dtype=dtype), tfp.layers.DistributionLambda(lambda t: tfd.Independent( tfd.Normal(loc=t[..., :n], scale=1e-5 + tf.nn.softplus(c + t[..., n:])), reinterpreted_batch_ndims=1)), ]) # Specify the prior over `keras.layers.Dense` `kernel` and `bias`. def prior_ridge(kernel_size, bias_size, dtype=None): return lambda _: tfd.Independent( tfd.Normal(loc=tf.zeros(kernel_size + bias_size), scale=tf.concat([2*tf.ones(kernel_size), 4*tf.ones(bias_size)], axis=0)), reinterpreted_batch_ndims=1 ) def build_model(context_dim, n_arms, sample_size, learning_rate=0.01): model = ThompsonLogistic(context_dim, n_arms, sample_size) # the loss function is the negloglik of the outcome y[:,1] and the head corresponding # to the arm assignment y[:,0] is selected with a one-hot mask loss_fn = lambda y, rv_y: tf.reduce_sum(-rv_y.log_prob(y[:,1, tf.newaxis]) * tf.one_hot(y[:,0], n_arms), axis=-1) model.compile(optimizer=tf.optimizers.Adam(learning_rate=learning_rate), loss=loss_fn) model.build(input_shape=(None, context_dim)) return model . . Learning strategy . In the learning phase of the model, at each step a new context $x_i$ is observed (or drawn from the population), an action $a_i$ is chosen, a reward $r_i$ is observed (or simulated with bandit.pull), and the model is updated. . class BayesianStrategy(object): &quot;&quot;&quot; Implements an online, learning strategy to solve the contextual multi-armed bandit problem. parameters: bandit: an instance of the ContextualBandit class methods: thompson_randomize(X): draws logits from the posterior distribution and returns the arms with the highest values _update_step(X, y): updates the model with the new observations one_trial(n, population): samples n elements from population, selects an arm for each of them through Thompson sampling, pulls it, updates the model train_on_data(X_train, all_outcomes_train): implements Thompson sampling on pre-sampled data where an omnicient being has pulled all the arms. The reason is to compare with standard Bayesian inference on the same data evaluate_training_decisions: returns statistics about action selection during training &quot;&quot;&quot; def __init__(self, bandit): self.bandit = bandit self.context_dim = bandit.shape[1] self.n_arms = bandit.shape[0] dtype = tf.float32 self.X = tf.cast(tf.reshape((), (0, self.context_dim)), tf.float32) self.y = tf.cast(tf.reshape((), (0, 2)), tf.int32) self.model = build_model(self.context_dim, self.n_arms, 1, learning_rate=0.008) self.loss = [] self.weights = [] def thompson_randomize(self, X): return tf.argmax(self.model.randomize(X), axis=1) def _update_step(self, X, y, epochs=10): self.X = tf.concat([self.X, X], axis=0) self.y = tf.concat([self.y, y], axis=0) weights = self.model.get_weights() self.model = build_model(self.context_dim, self.n_arms, self.X.shape[0], learning_rate=0.008) self.model.set_weights(weights) hist = self.model.fit(self.X, self.y, verbose=False, epochs=epochs) self.loss.append(hist.history[&#39;loss&#39;]) self.weights.append(self.model.get_weights_stats()) def one_trial(self, n, population, epochs=10): X = population.sample(n) selected_arms = self.thompson_randomize(X) outcomes = self.bandit.pull(selected_arms, X) y = tf.concat([tf.cast(selected_arms[:,tf.newaxis], tf.int32), outcomes[:,tf.newaxis]], axis=1) self._update_step(X, y, epochs) def train_on_data_step(self, X, all_outcomes, epochs): selected_arms = self.thompson_randomize(X) column_indices = tf.convert_to_tensor(selected_arms, dtype=tf.int64) row_indices = tf.range(X.shape[0], dtype=tf.int64) full_indices = tf.stack([row_indices, column_indices], axis=1) outcomes = tf.gather_nd(all_outcomes, full_indices) y = tf.concat([tf.cast(selected_arms[:,tf.newaxis], tf.int32), outcomes[:,tf.newaxis]], axis=1) self._update_step(X, y, epochs) def train_on_data(self, X_train, all_outcomes_train, batch_size=1, epochs=10): n_train = X_train.shape[0] ds = tf.data.Dataset.from_tensor_slices((X_train, all_outcomes_train)).batch(batch_size) for (X, all_outcomes) in ds: self.train_on_data_step(X, all_outcomes, epochs) def train_on_data_standard(self, X_train, all_outcomes_train, epochs=1000): n_train = X_train.shape[0] n_zeros = n_train//2 n_ones = n_train - n_zeros selected_arms = tf.cast(tf.math.floormod(tf.range(n_train), 2), tf.int64) column_indices = tf.convert_to_tensor(selected_arms, dtype=tf.int64) row_indices = tf.range(n_train, dtype=tf.int64) full_indices = tf.stack([row_indices, column_indices], axis=1) outcomes_train = tf.gather_nd(all_outcomes_train, full_indices) y_train = tf.concat([tf.cast(selected_arms[:,tf.newaxis], tf.int32), outcomes_train[:,tf.newaxis]], axis=1) self._update_step(X_train, y_train, epochs) def evaluate_training_decisions(self): best_arm_proportion = tf.reduce_mean(tf.cast( tf.cast(self.y[:,0], tf.int64)==self.bandit.get_optimal_arm(self.X), tf.float32)).numpy() success_rate = self.y[:,1].numpy().sum()/self.y.shape[0] prob_of_success = tf.reduce_mean(self.bandit.get_selected_probs(tf.cast(self.y[:,0], tf.int64), self.X), axis=0).numpy() return {&#39;training_best_arm_proportion&#39;: best_arm_proportion, &#39;training_success_rate&#39;: success_rate, &#39;training_prob_of_success&#39;: prob_of_success } . . After 60 to 80 iterations, the surrogate posteriors seem to have converged to distributions that are compatible with the true values of the parameters. . For comparison, we can train models on the same sample that has been assigned purely randomly to each arm. . The surrogate posterior distributions look similar to the ones obtained from Thompson sampling, and the predictive performance on a test set are comparable. In the following table, &quot;best_arm_selection_rate&quot; describes how frequently the best action is selected for contexts in the test set according to the predictions of the two models, and &quot;model_prob_of_success&quot; is the average of the true probabilities of success for the actions selected by the model. For reference, &quot;arms_probs_of_success&quot; shows the average of the true probabilities of success for each action in the case it is always picked. The benefit of Thompson sampling is revealed in the predictive performance during training. In the same table, &quot;training_best_arm_proportion&quot; indicates how often the best action is selected during training (as expected, roughly half the time for standard randomization), &quot;training_success_rate&quot; the observed success rate during training and &quot;training_prob_of_success&quot; the average probability of success following the assignment decisions made during training. . Thompson randomization Standard randomization . training_best_arm_proportion 0.775 | 0.5875 | . training_success_rate 0.4375 | 0.375 | . training_prob_of_success 0.448006 | 0.357196 | . best_arm_selection_rate 0.9228 | 0.9126 | . model_prob_of_success 0.48482 | 0.484251 | . arms_probs_of_success [0.35353488, 0.27571228] | [0.35353488, 0.27571228] | . In terms of reward, it is clear that training a model with Thompson randomization costs less than with standard randomization, and the inferred arm selection policy after training is very similar. In a clinical trial, that would translate into more enrolled subjects getting the best therapy according to their biomarkers. . Tougher bandits . The simple model presented in this note can be expanded in several directions. We can obviously consider more arms and contexts of higher dimensions. In that case, incorporating expert knowledge in the form of more informative priors or more complex surrogate posteriors can be useful. We can also include past observations to achieve faster convergence, and tamper them with lower weights if they are less relevant than the data sampled from the bandits during training. This type of jump start is particularly relevant in fields like online advertising where lower overall probabilities require more observations. . More complex mechanisms could be modeled with deeper Bayesian neural networks. The important requirement is a layer that can implement Thompson sampling. Moreover, the DistributionLambda top layer is not limited to Bernoulli distributions, and a wide variety of reward distributions can be easily simulated. It is probably reasonable to start with a single DenseVariational layer with adequate priors and variational surrogate posteriors with a top DistributionLambda layer compatible with the rewards, and then try to add layers to improve performance. As in most machine learning problems, the key is experimentation. .",
            "url": "https://ybarmaz.github.io/blog/contextual%20bandits/reinforcement%20learning/bayesian%20modeling/variational%20inference/probabilistic%20machine%20learning/tensorflow-probability/2021/02/09/Contextual_bandits.html",
            "relUrl": "/contextual%20bandits/reinforcement%20learning/bayesian%20modeling/variational%20inference/probabilistic%20machine%20learning/tensorflow-probability/2021/02/09/Contextual_bandits.html",
            "date": " • Feb 9, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Variational inference with TensorFlow-Probability",
            "content": "If you plan to automate the execution of multible Bayesian inference jobs, typically to regularly update your posterior distribution as new data comes in, you might find that MCMC algorithms take too long to sample their chains. Variational inference can speed things up considerably (you can find a good introduction here), at the expense of converging only to an approximation of the true posterior, which is often good enough for practical applications. . Lately, I have been experimenting with TensorFlow-Probability variational inference tools, namely tfp.vi.fit_surrogate_posterior and tfp.layers, to see how to integrate them into some of my projects. . I collected insights from various guides, tutorials, and code documentation, that I am summarizing here, mainly for future reference, but also for the benefit of people I will manage to convert to Bayesianism. The code is applied to a toy example of Bayesian logistic regression on simulated data, because it is helpful in this context to compare results to the true parameters of the data generating process. . true_params = np.array([1., 0., -2.]) true_offset = 1. true_generating_process = tfd.JointDistributionSequentialAutoBatched([ # features tfd.Sample(tfd.Normal(loc=0., scale=1.), true_params.shape[0]), # observations lambda features: tfd.Bernoulli(logits=true_offset + tf.tensordot(features, tf.convert_to_tensor( true_params, dtype=tf.float32), axes=1)) ]) [X, y] = true_generating_process.sample(500) . MCMC . It is always good to start with a benchmark, so I collected an MCMC sample of the posterior distribution and computed the means and standard deviations of the model parameters. . # Specify the model for Bayesian logistic regression. mdl_logreg = tfd.JointDistributionSequentialAutoBatched([ #betas tfd.Sample(tfd.Normal(loc=0., scale=5.), X.shape[1]), #alpha tfd.Normal(loc=0., scale=20.), #observations lambda alpha, betas: tfd.Independent( tfd.Bernoulli(logits=alpha + tf.tensordot(X, betas, axes=1)), reinterpreted_batch_ndims=1) ]) # Specify the MCMC algorithm. dtype = tf.dtypes.float32 nchain = 5 b0, a0, _ = mdl_logreg.sample(nchain) init_state = [b0, a0] step_size = [tf.cast(i, dtype=dtype) for i in [.1, .1]] target_log_prob_fn = lambda *init_state: mdl_logreg.log_prob( list(init_state) + [y]) # bijector to map contrained parameters to real unconstraining_bijectors = [ tfb.Identity(), tfb.Identity(), ] @tf.function(autograph=False, experimental_compile=True) def run_chain(init_state, step_size, target_log_prob_fn, unconstraining_bijectors, num_steps=8000, burnin=1000): def trace_fn(_, pkr): return ( pkr.inner_results.inner_results.target_log_prob, pkr.inner_results.inner_results.leapfrogs_taken, pkr.inner_results.inner_results.has_divergence, pkr.inner_results.inner_results.energy, pkr.inner_results.inner_results.log_accept_ratio ) kernel = tfp.mcmc.TransformedTransitionKernel( inner_kernel=tfp.mcmc.NoUTurnSampler( target_log_prob_fn, step_size=step_size), bijector=unconstraining_bijectors) hmc = tfp.mcmc.DualAveragingStepSizeAdaptation( inner_kernel=kernel, num_adaptation_steps=burnin, step_size_setter_fn=lambda pkr, new_step_size: pkr._replace( inner_results=pkr.inner_results._replace(step_size=new_step_size)), step_size_getter_fn=lambda pkr: pkr.inner_results.step_size, log_accept_prob_getter_fn=lambda pkr: pkr.inner_results.log_accept_ratio ) chain_state, sampler_stat = tfp.mcmc.sample_chain( num_results=num_steps, num_burnin_steps=burnin, current_state=init_state, kernel=hmc, trace_fn=trace_fn) return chain_state, sampler_stat # Run the chain samples, sampler_stat = run_chain( init_state, step_size, target_log_prob_fn, unconstraining_bijectors) # using the pymc3 naming convention sample_stats_name = [&#39;lp&#39;, &#39;tree_size&#39;, &#39;diverging&#39;, &#39;energy&#39;, &#39;mean_tree_accept&#39;] sample_stats = {k:v.numpy().T for k, v in zip(sample_stats_name, sampler_stat)} sample_stats[&#39;tree_size&#39;] = np.diff(sample_stats[&#39;tree_size&#39;], axis=1) var_name = [&#39;beta&#39;, &#39;alpha&#39;] posterior = {k:np.swapaxes(v.numpy(), 1, 0) for k, v in zip(var_name, samples)} az_trace = az.from_dict(posterior=posterior, sample_stats=sample_stats) az.plot_trace(az_trace) plt.show() . Variational inference with tfp.vi . The dedicated tool for variational inference in TensorFlow-Probability, tfp.vi.fit_surrogate_posterior, requires a similar amount of preparatory work as tfp.mcmc algorithms. The specification of the target posterior is actually the same. . mdl_logreg = tfd.JointDistributionSequentialAutoBatched([ #betas tfd.Sample(tfd.Normal(loc=0., scale=5.), X.shape[1]), #offset tfd.Normal(loc=0., scale=20.), #observations lambda offset, betas: tfd.Independent( tfd.Bernoulli(logits=offset + tf.tensordot(X, betas, axes=1)), reinterpreted_batch_ndims=1) ]) unnormalized_log_prob = lambda *x: mdl_logreg.log_prob(x + (y,)) . Then, instead of specifying a Markov chain, we have to define a variational family of surrogate posterior candidates. This can require quite a bit of work, but if our model has been built with a joint distribution list and we are happy with a mean field approximation (this is usually the case if we care only about the marginal posterior distributions of the individual model parameters and not their correlation), the TensorFlow tutorial on modeling with joint distributions provides a helper function to do that. Note that if the support of the distributions is not a full $ mathbb{R}^n$, we have to implement unconstraining bijectors. The same tutorial shows how to do it. . # Build meanfield ADVI for a jointdistribution # Inspect the input jointdistribution and replace the list of distribution with # a list of Normal distribution, each with the same shape. def build_meanfield_advi(jd_list, observed_node=-1): &quot;&quot;&quot; The inputted jointdistribution needs to be a batch version &quot;&quot;&quot; # Sample to get a list of Tensors list_of_values = jd_list.sample(1) # &lt;== sample([]) might not work # Remove the observed node list_of_values.pop(observed_node) # Iterate the list of Tensor to a build a list of Normal distribution (i.e., # the Variational posterior) distlist = [] for i, value in enumerate(list_of_values): dtype = value.dtype rv_shape = value[0].shape loc = tf.Variable( tf.random.normal(rv_shape, dtype=dtype), name=&#39;meanfield_%s_mu&#39; % i, dtype=dtype) scale = tfp.util.TransformedVariable( tf.fill(rv_shape, value=tf.constant(0.02, dtype)), tfb.Softplus(), name=&#39;meanfield_%s_scale&#39; % i, ) approx_node = tfd.Normal(loc=loc, scale=scale) if loc.shape == (): distlist.append(approx_node) else: distlist.append( # TODO: make the reinterpreted_batch_ndims more flexible (for # minibatch etc) tfd.Independent(approx_node, reinterpreted_batch_ndims=1) ) # pass list to JointDistribution to initiate the meanfield advi meanfield_advi = tfd.JointDistributionSequential(distlist) return meanfield_advi . It remains to choose an optimizer and set a few hyperparameters such as the number of optimization steps, the sample size used to estimate the loss function, and the learning rate of the optimizer. To better tune those and then to assess convergence, it can be helpful to enrich the trace function with statistics of the variational distribution. . meanfield_advi = build_meanfield_advi(mdl_logreg, observed_node=-1) # Check the logp and logq advi_samples = meanfield_advi.sample(4) print([ meanfield_advi.log_prob(advi_samples), unnormalized_log_prob(*advi_samples) ]) # Specify a trace function that collects statistics during inference and an optimizer trace_fn = lambda x: (x.loss, meanfield_advi.mean(), meanfield_advi.stddev()) opt = tf.optimizers.Adam(learning_rate=.08) #@tf.function(experimental_compile=True) def run_approximation(): loss_ = tfp.vi.fit_surrogate_posterior( unnormalized_log_prob, surrogate_posterior=meanfield_advi, optimizer=opt, sample_size=50, num_steps=200, trace_fn=trace_fn ) return loss_ loss_, q_mean_, q_std_ = run_approximation() . The loss itself is obviously a good indicator of convergence, but the model parameters seem to need a few more iterations to reach a steady state of the optimizer. . The standard deviation estimates exhibit some more noise. . To reduce the noise, one can try to increase the sample size used to estimate the loss function, or decrease the learning rate. The price for both actions is a slower convergence, so the number of iterations would need to be adjusted accordingly. Implementing a learning rate schedule could offer a trade-off. . Variational inference with tfp.layers . If the model can be cast as a neural network with a prior distribution on the neuron parameters, chances are it can be expressed as a Keras model with dedicated tfp.layers, a TensorFlow tool for probabilistic machine learning. . The probabilistic layers tutorial covers a least squares regression example in details and was a good inspiration, especially the case 4: aleatoric &amp; epistemic uncertainty. The biggest difference is in the specification of the prior, to which they assign learnable parameters. . To understand how to build a probabilistic machine learning model, we can start with the neural network expression of the logistic regression, namely a single neuron with a sigmoid activation, and &quot;probabilize&quot; it. . tfk = tf.keras classical_model = tf.keras.Sequential([ tfk.layers.Dense(1), tfk.layers.Activation(&#39;sigmoid&#39;) ]) classical_model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.01), loss=tfk.losses.BinaryCrossentropy() ) classical_model.fit(X, y, epochs=50) . The output of this network is the probability parameter of a Bernoulli distribution that is fit to the observed data through minimization of the binary cross-entropy. The activation layer can be replaced with a tfp.layers.DistributionLambda layer that outputs a tfd.Bernoulli distribution directly, which can be fit through minimization of the negative log-likelihood (note that the activation can be skipped if we use the logit parameter). . The parameters of the logistic regression are encoded in the tfk.layers.Dense layer. Its probabilistic version, tfp.layers.DenseVariational, also specifies a prior distribution for these parameters as well as a variational family to estimate their posterior. . # Define the negative log likelihood loss function for the `DistributionLambda` # head of the model. negloglik = lambda y, rv_y: -rv_y.log_prob(y) # Define a function to constrain the scale parameters to the real positive def constrain_scale(x): c = np.log(np.expm1(1.)) return 1e-5 + tf.nn.softplus(c + x) # Specify the surrogate posterior over `keras.layers.Dense` `kernel` and `bias`. def posterior_mean_field(kernel_size, bias_size, dtype=None): n = kernel_size + bias_size return tf.keras.Sequential([ tfp.layers.VariableLayer(2 * n, dtype=dtype), tfp.layers.DistributionLambda(lambda t: tfd.Independent( tfd.Normal(loc=t[..., :n], scale=constrain_scale(t[..., n:]) #scale=1e-5 + tf.nn.softplus(c + t[..., n:])), ), reinterpreted_batch_ndims=1)), ]) # Specify the prior over `keras.layers.Dense` `kernel` and `bias`. def prior_ridge(kernel_size, bias_size, dtype=None): return lambda _: tfd.Independent( tfd.Normal(loc=tf.zeros(kernel_size + bias_size), scale=tf.concat([5*tf.ones(kernel_size), 20*tf.ones(bias_size)], axis=0)), reinterpreted_batch_ndims=1 ) # Specify the model probabilistic_model = tf.keras.Sequential([ tfp.layers.DenseVariational(units=1, make_posterior_fn=posterior_mean_field, make_prior_fn=prior_ridge, kl_weight=1/X.shape[0] ), tfp.layers.DistributionLambda(lambda t: tfd.Bernoulli(logits=t)), ]) . To understand the kl_weight argument of tfp.layers.DenseVariational, we need to take a look at the mathematics behind variational inference that has been so far left out of this discussion. To estimate the posterior distribution $P(Z vert X)$ of the parameters $Z$ given the data $X$, the variational inference algorithms implemented here look for the distribution $Q(Z)$ in the variational family that minimizes the Kullback-Leibler divergence . $$ D_{ mathrm{KL}}(Q Vert P) = mathbb{E}_Q left[ log frac{Q(Z)}{P(Z vert X)} right] $$of $P(Z vert X)$ from $Q(Z)$. This quantity still depends on the unknown posterior $P(Z vert X)$, but minimizing it is equivalent to maximizing the evidence lower bound . $$ mathrm{ELBO} = mathbb{E}_Q left[ log P(X vert Z) + log P(Z) - log Q(Z) right] $$which depends only on the likelihood, the prior, and the variational distributions. . We prefer minimization problems, so we consider the negative ELBO loss function, and we make the dependency on individual data points explicit, . $$ - mathrm{ELBO} = mathbb{E}_Q left[ sum_i left( - log P(X_i vert Z) + frac{1}{N}( log Q(Z) - log P(Z)) right) right] $$where $N$ is the number of data points. The first term in the sum corresponds to the negative log-likelihood passed as the loss argument to probabilistic_model.compile. The rest of the sum is a regularization implemented in tfp.layers.DenseVariational, where $Q(Z)$ corresponds to the make_posterior_fn argument, $P(Z)$ to make_prior_fn, and $N$ to kl_weight=1/X.shape[0]. . As a side note, the sample size argument in tfp.vi.fit_surrogate_posterior gives the number of points sampled from $Q$ to compute a Monte Carlo estimate of the negative ELBO. . With the Keras API, we can use callbacks to collect training statistics or implement early stopping policies, and we have access to the tf.data.Dataset API for batch training (kl_weight would need to be adapted accordingly). Here we use callbacks to record the parameters of the variational distribution during training. . def get_model_stats(probabilistic_model): weights = probabilistic_model.layers[0].weights[0] k = X.shape[1] locs = weights[:k+1].numpy() scales = constrain_scale(weights[k+1:]).numpy() return locs, scales params_history = [] params_callback = tfk.callbacks.LambdaCallback( on_epoch_end=lambda epoch, logs: params_history.append( np.array(get_model_stats(probabilistic_model)))) probabilistic_model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.01), loss=negloglik ) history = probabilistic_model.fit(X, y, epochs=200, callbacks=[params_callback]) . Convergence of the standard deviation parameters is a bit slow but could be improved with a more informed choice of starting values. . Conclusion . The means and standard deviations of the posterior distributions inferred with tfp.mcmc, tfp.vi and tfp.layers (bnn) are summarized in the following table. . true_value mcmc_mean mcmc_std vi_mean vi_std bnn_mean bnn_std . parameter . beta_0 1.000 | 1.095 | 0.145 | 1.094 | 0.125 | 1.108 | 0.115 | . beta_1 0.000 | 0.079 | 0.122 | 0.084 | 0.115 | 0.055 | 0.120 | . beta_2 -2.000 | -1.840 | 0.184 | -1.838 | 0.163 | -1.835 | 0.146 | . alpha 1.000 | 1.223 | 0.142 | 1.203 | 0.131 | 1.218 | 0.124 | . The results are very similar across the three methods. Convergence is quite faster with variational inference, but it requires a bit more work to specify sensible variational families. . While tfp.vi is applicable to a wider class of problems, tfp.layers gives access to Keras functionalities such as callbacks, and, more interestingly, batch training with the tf.data.Dataset API. . There is no definitive rule for which method to apply to which problem, but it is important to be aware of the limitations and benefits of variational inference algorithms before using them. In this example we knew the true parameters in advance, but in real applications, one should have validation procedures in place to ensure the variational family is large enough to capture the phenomenon of interest, for instance by comparison with MCMC results, or for prediction tasks with a test set where the labels/outcomes are known. .",
            "url": "https://ybarmaz.github.io/blog/bayesian%20modeling/variational%20inference/tensorflow-probability/2021/02/01/Variational-inference-with-tfp.html",
            "relUrl": "/bayesian%20modeling/variational%20inference/tensorflow-probability/2021/02/01/Variational-inference-with-tfp.html",
            "date": " • Feb 1, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Anomaly detection in protocol deviations",
            "content": "Deviations from the approved protocol of a clinical trial are common but need to be reported by investigators to the trial sponsor for review. Failing to do so can put patients at risk and affect the scientific quality of the trial, so detecting protocol deviation underreporting is part of the standard quality assurance activities. Investigator sites that report more deviations than their peers should also raise an alarm as this might indicate underlying quality issues. A statistical tool that quantifies the risk of over- and underreporting of protocol deviations could significantly improve quality activities, much like in the case of adverse event reporting. . The number of protocol deviations $n_{pdevs}$ reported by an investigator site should depend linearly on the number of patients $n_{pats}$ enrolled at that site, as more patients mean more chances of deviations, so these are the minimal attributes that should get collected prior to the analysis. The following table is a sample of the data we will use as an example. . site_id n_pats n_pdevs . 0 site_0 | 9 | 15 | . 1 site_1 | 10 | 14 | . 2 site_2 | 15 | 18 | . 3 site_3 | 9 | 10 | . 4 site_4 | 5 | 4 | . A quick glance at the full dataset reveals there is indeed a relationship that looks linear, so a potential approach would be to build a regression model $n_{pdevs} sim theta cdot n_{pats}$ and quantify how every observation of $n_{pdevs}$ deviates from its estimation. . From that scatterplot, it also appears that the residuals of a regression would not be iid. Rather, their variance would grow as the number of patients increases. This rules out least square regression, as it assumes iid normal residuals. Since we are dealing with count data, working with the Poisson distribution is a natural approach, . $$n_{pdevs} vert n_{pats} sim Poi( lambda(n_{pats})),$$ . and we can set $ lambda(n_{pats}) = theta cdot n_{pats}$ to reflect our assumption of a linear relationship between $n_{pdevs}$ and $n_{pats}$. Note that a regular Poisson regression would fail to capture that linear relationship due to its exponential link function. . In this model, we immediately have $E left[n_{pdevs} vert n_{pats} right] = Var left[n_{pdevs} vert n_{pats} right] = theta cdot n_{pats}$, which seems to reproduce the increasing spread of $n_{pdevs}$. . We can infer the value of $ theta$ through maximum likelihood estimation and use the resulting conditional Poisson model at each site to compute the cumulative distribution function (CDF) of the observed numbers of protocol deviations. . def loss(par, n_pat, n_dev): theta = tf.math.exp(par[0]) dist = tfd.Poisson(n_pat * theta) return -tf.reduce_sum(dist.log_prob(n_dev)) def compute_cdf(par, n_pat, n_dev): theta = tf.math.exp(par[0]) dist = tfd.Poisson(n_pat * theta) return dist.cdf(n_dev) @tf.function def loss_and_gradient(par, n_pat, n_dev): return tfp.math.value_and_gradient(lambda par: loss(par, n_pat, n_dev), par) def fit(n_pat, n_dev): init = 2*tf.ones(1) opt = tfp.optimizer.lbfgs_minimize( lambda par: loss_and_gradient(par, n_pat, n_dev), init, max_iterations=1000 ) return opt n_pats = tf.constant(data[&#39;n_pats&#39;], dtype=tf.float32) n_pdev = tf.constant(data[&#39;n_pdevs&#39;], dtype=tf.float32) mle = fit(n_pats, n_pdev) #print(f&quot;converged: {mle.converged}&quot;) #print(f&quot;iterations: {mle.num_iterations}&quot;) x = np.linspace(0, 40) par = mle.position y = np.exp(par[0]) * x cdfs = compute_cdf(par, n_pats, n_pdev) . . These CDF values are concentrated around 0 and 1, which makes this approach quite impractical and suggests that the variance of the model is lower than the variance of the data. . The low variance can be increased by treating $ lambda(n_{pats})$ as a random function, rather than a deterministic one. So we assume that $ lambda(n_{pats})$ is drawn from a gamma distribution, $ lambda(n_{pats}) sim Gamma( alpha, beta)$, where the rate parameter $ beta$ is inversely proportional to the expected number of protocol deviations from a given site, $ beta = beta_{pat} / n_{pats}$, in order to ensure linearity in $n_{pats}$. In this context, maximum likelihood estimation would be a nightmare to implement (because of the rate parameters of the Poisson distribution) and probably not very stable, so it is best to turn to Bayesian inference via MCMC algorithms. We thus pick gamma priors for $ alpha$ and $ beta_{pat}$ with a shape parameters of 2 to prevent the corresponding Markov chains from drifting too close to zero, where pathological behaviors seem to occur with more permissive priors in this model. . sites = tf.constant(data[&#39;site_id&#39;]) n_pats = tf.constant(data[&#39;n_pats&#39;], dtype=tf.float32) n_pdev = tf.constant(data[&#39;n_pdevs&#39;], dtype=tf.float32) mdl_pd = tfd.JointDistributionSequential([ #alpha tfd.Gamma(2, 2, name=&#39;alpha&#39;), #beta_pt tfd.Gamma(2, 2, name=&#39;beta_pt&#39;), #pdev rates for each patient lambda beta_pt, alpha: tfd.Independent( tfd.Gamma(alpha[...,tf.newaxis], beta_pt[...,tf.newaxis] / n_pats[tf.newaxis,...]), reinterpreted_batch_ndims=1 ), #observed pdevs lambda rates: tfd.Independent(tfd.Poisson(rates), reinterpreted_batch_ndims=1) ]) . . We can sample the posterior distribution of this model with a Hamiltonian Monte Carlo algorithm and assess the convergence of the Markov chains before computing the posterior probabilities of interest. . dtype = tf.dtypes.float32 nchain = 5 burnin=3000 num_steps=10000 alpha0, beta_pt0, rates0, _ = mdl_pd.sample(nchain) init_state = [alpha0, beta_pt0, rates0] step_size = [tf.cast(i, dtype=dtype) for i in [0.01, 0.01, 0.01]] target_log_prob_fn = lambda *init_state: mdl_pd.log_prob( list(init_state) + [tf.cast(n_pdev, dtype=dtype)]) unconstraining_bijectors = 3*[tfb.Exp()] @tf.function(autograph=False, experimental_compile=True) def run_chain(init_state, step_size, target_log_prob_fn, unconstraining_bijectors, num_steps=num_steps, burnin=burnin): def trace_fn(_, pkr): return ( pkr.inner_results.inner_results.is_accepted ) kernel = tfp.mcmc.TransformedTransitionKernel( inner_kernel=tfp.mcmc.HamiltonianMonteCarlo( target_log_prob_fn, num_leapfrog_steps=3, step_size=step_size), bijector=unconstraining_bijectors) hmc = tfp.mcmc.SimpleStepSizeAdaptation( inner_kernel=kernel, num_adaptation_steps=burnin ) # Sampling from the chain. [alpha, beta_pt, rates], is_accepted = tfp.mcmc.sample_chain( num_results=num_steps, num_burnin_steps=burnin, current_state=init_state, kernel=hmc, trace_fn=trace_fn) return alpha, beta_pt, rates, is_accepted alpha, beta_pt, rates, is_accepted = run_chain( init_state, step_size, target_log_prob_fn, unconstraining_bijectors) alpha_ = alpha[burnin:,:] alpha_ = tf.reshape(alpha_, [alpha_.shape[0]*alpha_.shape[1]]) beta_pt_ = beta_pt[burnin:,:] beta_pt_ = tf.reshape(beta_pt_, [beta_pt_.shape[0]*beta_pt_.shape[1]]) rates_ = rates[burnin:,:] rates_ = tf.reshape(rates_, [rates_.shape[0]*rates_.shape[1], rates_.shape[2]]) rates_dist_ = tfd.Gamma(alpha_[:,tf.newaxis], beta_pt_[:, tf.newaxis] / n_pats[tf.newaxis,...]) rates_cdf_ = rates_dist_.cdf(rates_) posterior = {} posterior[&#39;alpha&#39;] = tf.transpose(alpha[burnin:, :]).numpy() posterior[&#39;beta_pt&#39;] = tf.transpose(beta_pt[burnin:, :]).numpy() posterior[&#39;rate0&#39;] = tf.transpose(rates[burnin:, :, 0]) posterior[&#39;rate1&#39;] = tf.transpose(rates[burnin:, :, 1]) posterior[&#39;rate2&#39;] = tf.transpose(rates[burnin:, :, 2]) az_trace = az.from_dict(posterior=posterior) print(f&#39;MCMC acceptance rate: {is_accepted.numpy().mean()}&#39;) az.plot_trace(az_trace) plt.show() . . MCMC acceptance rate: 0.73832 . Given a Markov chain sample $( hat alpha, hat beta_{pat}, ( hat lambda_i)_{i=1, dots,N})$, where $i$ indexes the investigator sites, we can evaluate the CDF of $ Gamma( hat alpha, hat beta_{pat} / n_{pats, i})$ at $ hat lambda_i$ and average these quantities along the whole Markov chain to obtain an indicator of over- and underreporting. This indicator corresponds to the rate tail area of the inferred Poisson rates under their posterior predictive distribution. Low values mean a risk of underreporting, and high values a risk of overreporting (see the last column of the following sample table). . site n_pats n_pdev mean_pdev_rate std_pdev_rate rate_tail_area . 0 site_0 | 9 | 15 | 15.51 | 3.82 | 0.42 | . 1 site_1 | 10 | 14 | 14.56 | 3.72 | 0.36 | . 2 site_2 | 15 | 18 | 18.73 | 4.28 | 0.31 | . 3 site_3 | 9 | 10 | 10.77 | 3.19 | 0.30 | . 4 site_4 | 5 | 4 | 4.86 | 2.08 | 0.24 | . 5 site_5 | 7 | 19 | 18.98 | 4.26 | 0.61 | . 6 site_6 | 25 | 31 | 31.74 | 5.59 | 0.32 | . 7 site_7 | 2 | 11 | 9.82 | 2.83 | 0.82 | . 8 site_8 | 6 | 7 | 7.75 | 2.71 | 0.32 | . 9 site_9 | 17 | 27 | 27.65 | 5.17 | 0.40 | . The distribution of the rate tail areas looks more convenient than in the first simple model. Not only did we add variance with a mixture model, but we also assess the inferred Poisson parameters rather than the observations, and the former are shrunk by their prior. . This metric also seems to agree with the intuition of what underreporting and overreporting should look like. . We can set thresholds for over- and underreporting alerts at .8 and .2 respectively to illustrate how an auditor could use this model to select investigator sites to focus on. . This method illustrates the potential of Bayesian modeling to supercharge a regression analysis toolbox with a variety of likelihood functions that can capture the intricacies of the data generating process and inference methods that are more flexible in quantifying uncertainties than the standard GLM methods. These properties are particularly helpful in practical tasks such as anomaly detection or risk management that combine expert insights with quantitative modeling. .",
            "url": "https://ybarmaz.github.io/blog/bayesian%20modeling/clinical%20quality/2021/01/13/Anomaly-detection-in-protocol-deviations.html",
            "relUrl": "/bayesian%20modeling/clinical%20quality/2021/01/13/Anomaly-detection-in-protocol-deviations.html",
            "date": " • Jan 13, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Probabilistic assessment of safety underreporting",
            "content": "This post presents a practical application of probabilistic programming and explores some technical details of a recent preprint where Timothé Ménard and I proposed a Bayesian model for the detection of adverse event underreporting in clinical trials. . A clinical trial is usually run across several investigator sites, which are required to report to the trial sponsor adverse events experienced by enrolled subjects. This is necessary to assess the safety of the intervention under investigation. Underreporting from certain sites has been a recurrent issue. Trial sponsors and health authorities rely on audits and inspections to ensure completeness of the collected safety data. This effort can and should be informed by statistical analysis of the adverse event reporting process with a focus on identifying sites that report at lower rates. . The main hurdle is that you are looking for missing data that you do not know is missing. We developed a solution around an observational model for adverse event reporting and a way to characterizing outlying investigator sites. . The relevant data is the count of adverse events reported by every patient enrolled in a study and their site assignment. To illustrate our methodology, we use data from the control arm of NCT00617669, an oncology study from Project Data Sphere. . Bayesian data analysis combines expert knowledge on the process of interest, expressed as a probabilistic model, with observed data to infer posterior distributions of the model parameters. These posterior distributions allow us to quantify uncertainties and risks, and to compute posterior expectation values of quantities of interest. . Hierarchical models provide a natural framework for subgroup analysis, where similar entities such as the sites of a single study can share information while maintaining a certain degree of independence. In our situation, at the bottom of the hierarchy, the count of adverse events reported by the $n_i$ patients of site $i$ can be modelled with a Poisson distribution, $Y_i sim mathrm{Poi}( lambda_i)$. The $N_{ mathrm{sites}}$ Poisson rates $ lambda_i$ can in turn be modelled as realizations of a random variable unique to the whole study with Gamma distribution $ Gamma( alpha, beta)$. The parameters $ alpha$ and $ beta$ are unknown, so we assume a vague prior for both of them, $ alpha sim mathrm{Exp}(1)$ and $ beta sim mathrm{Exp}(10)$. The full joint distribution $$ P( alpha, beta, lambda_i, Y_{i,j}) = P( alpha)P( beta) prod_{i=1}^{N_{ mathrm{sites}}}P( lambda_i vert alpha, beta) prod_{j=1}^{n_i} P(Y_{i,j} vert lambda_i) $$ is summarized in the following graphical representation. . . Numerical modelling of such joint distributions is made easy by probabilistic programming libraries such as TensorFlow-Probability (utilized here), Stan, PyMC3 or Pyro. . import tensorflow as tf import tensorflow_probability as tfp tfb = tfp.bijectors tfd = tfp.distributions sites = tf.constant(data[&#39;site_number&#39;]) observed_ae = tf.constant(data[&#39;ae_count_cumulative&#39;]) unique_sites, sites_idx, sites_counts = tf.unique_with_counts(sites) ae_per_site = tf.RaggedTensor.from_value_rowids( values=observed_ae, value_rowids=sites_idx) mdl_ae = tfd.JointDistributionSequential([ #alpha tfd.Gamma(1, 1, name=&#39;alpha&#39;), #beta tfd.Gamma(1, 10, name=&#39;beta&#39;), #Poisson rates for each sites lambda beta, alpha: tfd.Sample(tfd.Gamma(alpha, beta), sample_shape=unique_sites.shape, name=&#39;rates&#39;), #observed AEs lambda rates: tfd.Independent( tfd.Poisson(tf.gather(rates, sites_idx, axis=-1)), reinterpreted_batch_ndims=1, name=&#39;observations&#39;) ]) . In Bayesian inference, the analytical derivation of the posterior distribution is often impossible, but the same probabilistic programming libraries provide efficient implementations of MCMC algorithms that return samples of the posterior distribution. . dtype = tf.dtypes.float32 nchain = 10 burnin=1000 num_steps=10000 alpha0, beta0, rates0, _ = mdl_ae.sample(nchain) init_state = [alpha0, beta0, rates0] step_size = [tf.cast(i, dtype=dtype) for i in [.1, .1, .1]] target_log_prob_fn = lambda *init_state: mdl_ae.log_prob( list(init_state) + [tf.cast(observed_ae, dtype=dtype)]) # bijector to map contrained parameters to real unconstraining_bijectors = [ tfb.Exp(), tfb.Exp(), tfb.Exp() ] @tf.function(autograph=False, experimental_compile=True) def run_chain(init_state, step_size, target_log_prob_fn, unconstraining_bijectors, num_steps=num_steps, burnin=burnin): def trace_fn(_, pkr): return ( pkr.inner_results.inner_results.is_accepted ) kernel = tfp.mcmc.TransformedTransitionKernel( inner_kernel=tfp.mcmc.HamiltonianMonteCarlo( target_log_prob_fn, num_leapfrog_steps=3, step_size=step_size), bijector=unconstraining_bijectors) hmc = tfp.mcmc.SimpleStepSizeAdaptation( inner_kernel=kernel, num_adaptation_steps=burnin ) # Sampling from the chain. [alpha, beta, rates], is_accepted = tfp.mcmc.sample_chain( num_results=num_steps, num_burnin_steps=burnin, current_state=init_state, kernel=hmc, trace_fn=trace_fn) return alpha, beta, rates, is_accepted . . To assess convergence, we sample several chains that we can inspect visually (here with the ArviZ package) to make sure that they converge to the same distribution, mix well, and do not display pathological autocorrelations. . If we have to monitor several studies, we might want to automate that process. In that case, we can compute statistics of the sampled chains such as effective sample sizes or $ hat{R}$ and implement automatic checks, for instance that $ hat{R}$ is sufficiently close to 1. . mean sd hdi_3% hdi_97% mcse_mean mcse_sd ess_mean ess_sd ess_bulk ess_tail r_hat . 3001 3.193 | 1.246 | 1.081 | 5.565 | 0.022 | 0.016 | 3146.0 | 3146.0 | 2849.0 | 3764.0 | 1.0 | . 3002 3.050 | 0.616 | 1.964 | 4.250 | 0.006 | 0.004 | 11787.0 | 11787.0 | 11502.0 | 17056.0 | 1.0 | . 3003 6.043 | 1.695 | 3.084 | 9.280 | 0.022 | 0.015 | 6031.0 | 6031.0 | 5772.0 | 9765.0 | 1.0 | . 3004 12.764 | 2.030 | 9.037 | 16.628 | 0.014 | 0.010 | 19819.0 | 19819.0 | 19489.0 | 28664.0 | 1.0 | . alpha 1.774 | 0.223 | 1.364 | 2.192 | 0.003 | 0.002 | 6352.0 | 6352.0 | 6253.0 | 16128.0 | 1.0 | . beta 0.119 | 0.017 | 0.087 | 0.151 | 0.000 | 0.000 | 7576.0 | 7576.0 | 7441.0 | 18742.0 | 1.0 | . From the samples $( hat{ alpha}, hat{ beta}, hat{ lambda}_i)$ of the Markov chain, we can estimate the posterior risk of underreporting. One way to do it is to compute the left tail area of each $ lambda_i$ (remember that the index $i$ enumerates the sites) under the distribution $ Gamma( hat{ alpha}, hat{ beta})$ and average it along the trace of the Markov chain. This corresponds to the probability that a Poisson rate drawn randomly from the study level distribution falls below the inferred Poisson rate of site $i$, or, more explicitly, that a reference site from the same study would report less adverse events. . site mean_ae_rate std_ae_rate rate_tail_area observed_ae . 0 3001 | 3.20 | 1.25 | 0.09 | [4, 1] | . 1 3002 | 3.05 | 0.62 | 0.08 | [2, 2, 1, 2, 5, 5, 5, 1] | . 2 3003 | 6.04 | 1.69 | 0.22 | [7, 4] | . 3 3004 | 12.76 | 2.03 | 0.52 | [3, 27, 8] | . 4 3005 | 9.35 | 2.10 | 0.37 | [12, 6] | . 5 3006 | 3.69 | 1.32 | 0.11 | [2, 4] | . 6 3007 | 6.03 | 2.31 | 0.22 | [5] | . 7 3008 | 17.95 | 1.72 | 0.69 | [11, 4, 16, 31, 23, 23] | . 8 3009 | 6.52 | 1.76 | 0.24 | [6, 6] | . 9 3010 | 14.71 | 0.92 | 0.59 | [21, 10, 6, 17, 10, 7, 26, 19, 18, 1, 18, 23, ... | . A lower value of this rate tail area thus indicates a higher risk of underreporting. This metric can be used by auditors and inspectors to prioritize their activities. Moreover, since it is the probability of a specified event, one can immediately compare the rate tail areas of sites from different studies. This is especially interesting for quality programs that oversee several trials. . This approach demonstrates the flexibility of Bayesian methods to build models that answer specific questions about a given process. In this example, the reporting rates of the different sites are the quantities of interest, but they are unobserved and have to be inferred from the available data with a mathematical model. The user-friendly API of modern probabilistic programming libraries combined with efficient inference algorithms have been making this type of workflow much easier than in the past and will certainly fuel a broader adoption in sectors that have not been traditionally driven by quantitative insights. .",
            "url": "https://ybarmaz.github.io/blog/bayesian%20modeling/clinical%20quality/2021/01/08/Probabilistic-assessment-of-safety-underreporting.html",
            "relUrl": "/bayesian%20modeling/clinical%20quality/2021/01/08/Probabilistic-assessment-of-safety-underreporting.html",
            "date": " • Jan 8, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "My background is in theoretical physics (quantum fields and strings) and I now work as a data scientist in the pharmaceutical industry, focusing mostly on applications of quantitative methods to drug development and optimization of business processes. . In my spare time, I enjoy ski touring, mountain biking and cycling, especially in the Swiss Alps where I grew up. . In this blog, I am sharing learnings from my data science and applied mathematics journey, so hopefully you won’t get stuck to the same sticky places. . . Get in touch . You can email me at yves.barma@gmail.com if you add the missing z before @. . Selected publications . Bayesian modeling for the detection of adverse events underreporting in clinical trials (preprint, submitted to Drug Safety) | Enabling Data-Driven Clinical Quality Assurance: Predicting Adverse Event Reporting in Clinical Trials Using Machine Learning | Using Statistical Modeling for Enhanced and Flexible Pharmacovigilance Audit Risk Assessment and Planning | Chern-Simons Theory with Wilson Lines and Boundary in the BV-BFV Formalism | . . This website is powered by fastpages by Hamel Husain. .",
          "url": "https://ybarmaz.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://ybarmaz.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}